@article{bengio_consciousness_2019,
        title = {The {Consciousness} {Prior}},
        url = {http://arxiv.org/abs/1709.08568},
        abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
        urldate = {2021-02-06},
        journal = {arXiv:1709.08568 [cs, stat]},
        author = {Bengio, Yoshua},
        month = dec,
        year = {2019},
        note = {arXiv: 1709.08568},
        keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
        file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/SNAUUX3S/Bengio - 2019 - The Consciousness Prior.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/6AXFBJ6X/1709.html:text/html}
}

@article{duan_rl2_2016,
        title = {{RL}\${\textasciicircum}2\$: {Fast} {Reinforcement} {Learning} via {Slow} {Reinforcement} {Learning}},
        shorttitle = {{RL}\${\textasciicircum}2\$},
        url = {http://arxiv.org/abs/1611.02779},
        abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL\${\textasciicircum}2\$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL\${\textasciicircum}2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL\${\textasciicircum}2\$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL\${\textasciicircum}2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
        urldate = {2021-02-07},
        journal = {arXiv:1611.02779 [cs, stat]},
        author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
        month = nov,
        year = {2016},
        note = {arXiv: 1611.02779},
        keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
        annote = {Comment: 14 pages. Under review as a conference paper at ICLR 2017},
        file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/WUYBD6CM/Duan et al. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow Reinf.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/UMFUMTHZ/1611.html:text/html}
}

@article{vaswani_attention_2017,
        title = {Attention {Is} {All} {You} {Need}},
        url = {http://arxiv.org/abs/1706.03762},
        abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
        urldate = {2021-01-29},
        journal = {arXiv:1706.03762 [cs]},
        author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
        month = dec,
        year = {2017},
        note = {arXiv: 1706.03762},
        keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
        annote = {Comment: 15 pages, 5 figures},
        file = {arXiv Fulltext PDF:/home/liusida/Zotero/storage/ATY6TRP5/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/liusida/Zotero/storage/T9ZL3NRB/1706.html:text/html}
}
